#!/usr/bin/env python3
"""
ä¸¥æ ¼æ¸…ç†æ•°æ®æ®‹ç¼ºçš„CSVæ–‡ä»¶

æ›´ä¸¥æ ¼çš„æ¸…ç†æ ‡å‡†ï¼š
1. ä»·æ ¼æ•°æ®ç©ºå€¼æ¯”ä¾‹è¶…è¿‡30%
2. æ–‡ä»¶å¤§å°å°äº2KB
3. æ•°æ®è¡Œæ•°å°‘äº100è¡Œ
4. æ‰€æœ‰ä»·æ ¼æ•°æ®ä¸ºç©º
"""

import argparse
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import List, Tuple

import pandas as pd
from tqdm import tqdm


def strict_check_file(csv_path: Path, max_null_ratio: float = 0.3, min_rows: int = 100, min_size: int = 2048) -> Tuple[bool, str]:
    """ä¸¥æ ¼æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰é—®é¢˜"""
    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = csv_path.stat().st_size
        if file_size < min_size:
            return False, f"æ–‡ä»¶è¿‡å°({file_size}å­—èŠ‚ï¼Œå°äº{min_size}å­—èŠ‚)"
        
        # å°è¯•è¯»å–æ–‡ä»¶
        df = pd.read_csv(csv_path)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if df.empty:
            return False, "æ–‡ä»¶ä¸ºç©º"
        
        # æ£€æŸ¥è¡Œæ•°
        if len(df) < min_rows:
            return False, f"æ•°æ®è¡Œæ•°è¿‡å°‘({len(df)}è¡Œï¼Œå°‘äº{min_rows}è¡Œ)"
        
        # æ£€æŸ¥å¿…è¦åˆ—
        required_columns = ["date", "open", "high", "low", "close", "volume"]
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            return False, f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}"
        
        # æ£€æŸ¥ä»·æ ¼æ•°æ®æ˜¯å¦å…¨ä¸ºç©º
        price_columns = ["open", "high", "low", "close"]
        price_data_exists = any(
            not df[col].isna().all() 
            for col in price_columns 
            if col in df.columns
        )
        
        if not price_data_exists:
            return False, "æ‰€æœ‰ä»·æ ¼æ•°æ®ä¸ºç©º"
        
        # æ£€æŸ¥ç©ºå€¼æ¯”ä¾‹ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        total_rows = len(df)
        for col in price_columns:
            if col in df.columns:
                null_ratio = df[col].isna().sum() / total_rows
                if null_ratio > max_null_ratio:
                    return False, f"{col}åˆ—ç©ºå€¼æ¯”ä¾‹è¿‡é«˜({null_ratio:.1%}ï¼Œè¶…è¿‡{max_null_ratio:.1%})"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ä»·æ ¼æ•°æ®
        valid_price_rows = 0
        for _, row in df.iterrows():
            if any(pd.notna(row[col]) and row[col] > 0 for col in price_columns if col in df.columns):
                valid_price_rows += 1
        
        valid_ratio = valid_price_rows / total_rows
        if valid_ratio < (1 - max_null_ratio):
            return False, f"æœ‰æ•ˆä»·æ ¼æ•°æ®æ¯”ä¾‹è¿‡ä½({valid_ratio:.1%}ï¼Œä½äº{1-max_null_ratio:.1%})"
        
        return True, "æ•°æ®è´¨é‡è‰¯å¥½"
        
    except Exception as e:
        return False, f"è¯»å–å¤±è´¥: {str(e)}"


def strict_scan_and_clean(data_dir: Path, backup_dir: Path = None, dry_run: bool = False, 
                         max_null_ratio: float = 0.3, min_rows: int = 100, min_size: int = 2048) -> None:
    """ä¸¥æ ¼æ‰«æå¹¶æ¸…ç†é—®é¢˜æ–‡ä»¶"""
    
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # è·å–æ‰€æœ‰CSVæ–‡ä»¶ï¼ˆæ’é™¤å¸‚å€¼æ–‡ä»¶ï¼‰
    csv_files = [f for f in data_dir.glob("*.csv") if not f.name.startswith("mktcap_")]
    
    if not csv_files:
        print("ğŸ“ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“Š å‘ç° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    print(f"ğŸ” æ¸…ç†æ ‡å‡†:")
    print(f"  - ä»·æ ¼æ•°æ®ç©ºå€¼æ¯”ä¾‹è¶…è¿‡ {max_null_ratio:.1%}")
    print(f"  - æ–‡ä»¶å¤§å°å°äº {min_size} å­—èŠ‚")
    print(f"  - æ•°æ®è¡Œæ•°å°‘äº {min_rows} è¡Œ")
    print(f"  - æ‰€æœ‰ä»·æ ¼æ•°æ®ä¸ºç©º")
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    if backup_dir and not dry_run:
        backup_dir.mkdir(parents=True, exist_ok=True)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total": len(csv_files),
        "problematic": 0,
        "deleted": 0,
        "backed_up": 0,
        "errors": 0
    }
    
    problematic_files = []
    
    # æ‰«ææ–‡ä»¶
    print("\nğŸ” ä¸¥æ ¼æ‰«ææ–‡ä»¶è´¨é‡...")
    for csv_path in tqdm(csv_files, desc="æ‰«æè¿›åº¦"):
        is_good, reason = strict_check_file(csv_path, max_null_ratio, min_rows, min_size)
        
        if not is_good:
            stats["problematic"] += 1
            problematic_files.append((csv_path, reason))
    
    if not problematic_files:
        print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½ç¬¦åˆä¸¥æ ¼æ ‡å‡†ï¼Œæ— éœ€æ¸…ç†")
        return
    
    print(f"\nâš ï¸  å‘ç° {len(problematic_files)} ä¸ªé—®é¢˜æ–‡ä»¶:")
    for csv_path, reason in problematic_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  - {csv_path.name}: {reason}")
    if len(problematic_files) > 10:
        print(f"  ... è¿˜æœ‰ {len(problematic_files) - 10} ä¸ªæ–‡ä»¶")
    
    # æ¸…ç†æ–‡ä»¶
    if dry_run:
        print(f"\nğŸ” [æ¨¡æ‹Ÿæ¨¡å¼] å°†åˆ é™¤ {len(problematic_files)} ä¸ªæ–‡ä»¶")
    else:
        print(f"\nğŸ—‘ï¸  å¼€å§‹ä¸¥æ ¼æ¸…ç† {len(problematic_files)} ä¸ªé—®é¢˜æ–‡ä»¶...")
        
        for csv_path, reason in tqdm(problematic_files, desc="æ¸…ç†è¿›åº¦"):
            try:
                # å¤‡ä»½æ–‡ä»¶
                if backup_dir:
                    backup_path = backup_dir / csv_path.name
                    shutil.copy2(csv_path, backup_path)
                    stats["backed_up"] += 1
                
                # åˆ é™¤æ–‡ä»¶
                csv_path.unlink()
                stats["deleted"] += 1
                
            except Exception as e:
                print(f"âŒ å¤„ç† {csv_path.name} å¤±è´¥: {e}")
                stats["errors"] += 1
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“‹ ä¸¥æ ¼æ¸…ç†å®Œæˆç»Ÿè®¡:")
    print(f"  æ€»æ–‡ä»¶æ•°: {stats['total']}")
    print(f"  é—®é¢˜æ–‡ä»¶æ•°: {stats['problematic']}")
    print(f"  åˆ é™¤æ–‡ä»¶æ•°: {stats['deleted']}")
    if backup_dir:
        print(f"  å¤‡ä»½æ–‡ä»¶æ•°: {stats['backed_up']}")
        print(f"  å¤‡ä»½ä½ç½®: {backup_dir}")
    print(f"  é”™è¯¯æ•°: {stats['errors']}")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    if not dry_run and problematic_files:
        report_path = data_dir / f"strict_clean_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(f"ä¸¥æ ¼æ¸…ç†æŠ¥å‘Š - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"æ•°æ®ç›®å½•: {data_dir}\n")
            f.write(f"å¤‡ä»½ç›®å½•: {backup_dir}\n")
            f.write(f"æ¸…ç†æ ‡å‡†:\n")
            f.write(f"  - ä»·æ ¼æ•°æ®ç©ºå€¼æ¯”ä¾‹è¶…è¿‡ {max_null_ratio:.1%}\n")
            f.write(f"  - æ–‡ä»¶å¤§å°å°äº {min_size} å­—èŠ‚\n")
            f.write(f"  - æ•°æ®è¡Œæ•°å°‘äº {min_rows} è¡Œ\n")
            f.write(f"  - æ‰€æœ‰ä»·æ ¼æ•°æ®ä¸ºç©º\n\n")
            f.write("ç»Ÿè®¡ä¿¡æ¯:\n")
            for key, value in stats.items():
                f.write(f"  {key}: {value}\n")
            f.write("\né—®é¢˜æ–‡ä»¶åˆ—è¡¨:\n")
            for csv_path, reason in problematic_files:
                f.write(f"  {csv_path.name}: {reason}\n")
        
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    parser = argparse.ArgumentParser(description="ä¸¥æ ¼æ¸…ç†æ•°æ®æ®‹ç¼ºçš„CSVæ–‡ä»¶")
    parser.add_argument("--data-dir", default="./data", help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--backup-dir", help="å¤‡ä»½ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤ä¸ºdata_backup_strictï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="æ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…åˆ é™¤æ–‡ä»¶")
    parser.add_argument("--no-backup", action="store_true", help="åˆ é™¤æ—¶ä¸å¤‡ä»½æ–‡ä»¶")
    parser.add_argument("--max-null-ratio", type=float, default=0.3, help="æœ€å¤§ç©ºå€¼æ¯”ä¾‹é˜ˆå€¼ï¼ˆé»˜è®¤30%ï¼‰")
    parser.add_argument("--min-rows", type=int, default=100, help="æœ€å°è¡Œæ•°è¦æ±‚ï¼ˆé»˜è®¤100è¡Œï¼‰")
    parser.add_argument("--min-size", type=int, default=2048, help="æœ€å°æ–‡ä»¶å¤§å°è¦æ±‚ï¼ˆé»˜è®¤2048å­—èŠ‚ï¼‰")
    
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir)
    backup_dir = None
    
    if not args.no_backup:
        if args.backup_dir:
            backup_dir = Path(args.backup_dir)
        else:
            backup_dir = data_dir.parent / "data_backup_strict"
    
    print("ğŸ§¹ ä¸¥æ ¼æ•°æ®æ¸…ç†å·¥å…·")
    print("=" * 30)
    print(f"æ•°æ®ç›®å½•: {data_dir}")
    if backup_dir:
        print(f"å¤‡ä»½ç›®å½•: {backup_dir}")
    print(f"æ¨¡å¼: {'æ¨¡æ‹Ÿè¿è¡Œ' if args.dry_run else 'å®é™…æ¸…ç†'}")
    print()
    
    try:
        strict_scan_and_clean(
            data_dir, backup_dir, args.dry_run, 
            args.max_null_ratio, args.min_rows, args.min_size
        )
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")


if __name__ == "__main__":
    main()
